{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cmpe 545 - Term Project: Twitter Sentiment Classification\n",
    "\n",
    "**Student:** Gonul Ayci - 2016800003<br>\n",
    "**Instructor:** Prof. Ethem Alpaydin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique:\n",
    "I give a detailed information about the steps in the following cells. Briefly, I want to solve the problem using by the following steps:\n",
    "\n",
    "**Step 0:** Split the dataset into two groups such as train and test data,\n",
    "\n",
    "**Step 1:** Get a cleaned data by using the followings,\n",
    "* Tokenization\n",
    "* Punctuaton\n",
    "* Stopwords\n",
    "\n",
    "**Step 2:** Feature representation,\n",
    "* **Step 2.1:** Use n-gram (baseline) method such as unigram, and bigram. Then, to apply this method to a cleaned data.\n",
    "\n",
    "* **Step 2.2:** Use word-embedding (second method).\n",
    "\n",
    "**Step 3:** Feature extraction,\n",
    "\n",
    "**Step 4:** Dimensionality reduction,\n",
    "\n",
    "**Step 5:** Use Multi-layer perceptron to classify tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract: \n",
    "Sentiment analysis is one of the popular topic on Artificial Neural network (ANN), and also on Natural Language Processing (NLP). The task of classifying subjective statements in the text into the categories such as \"positive\", \"negative\" and \"neutral\". <br>\n",
    "\n",
    "The aim of this project is to perform sentiment analysis on twitter dataset which is available on http://help.sentiment140.com/for-students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk import ngrams\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data \n",
    "\n",
    "In this project, I use labeled data which is from http://help.sentiment140.com/for-students.\n",
    "\n",
    "The data is a CSV with emoticons removed. Data file format has 6 fields: <br>\n",
    "0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive) <br>\n",
    "1 - the id of the tweet (2087) <br>\n",
    "2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009) <br>\n",
    "3 - the query (lyx). If there is no query, then this value is NO_QUERY. <br>\n",
    "4 - the user that tweeted (robotickilldozr) <br>\n",
    "5 - the text of the tweet (Lyx is cool) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trainingandtestdata/testdata.manual.2009.06.14.csv')\n",
    "len(df[['polarity', 'tweet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>I was talking to this guy last night and he wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>i love lebron. http://bit.ly/PdHur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>@ludajuice Lebron is a Beast, but I'm still ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>@Pmillzz lebron IS THE BOSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>@sketchbug Lebron is a hometown hero to me, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>lebron and zydrunas are such an awesome duo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>@wordwhizkid Lebron is a beast... nobody in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>downloading apps for my iphone! So much fun :-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>good news, just had a call from the Visa offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>http://twurl.nl/epkr4b - awesome come back fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>In montreal for a long weekend of R&amp;amp;R. Muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>Booz Allen Hamilton has a bad ass homegrown so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>[#MLUC09] Customer Innovation Award Winner: Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>@SoChi2 I current use the Nikon D90 and love i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>Man I kinda dislike Apple right now. Case in p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>4</td>\n",
       "      <td>@cwong08 I have a Kindle2 (&amp;amp; Sony PRS-500)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>4</td>\n",
       "      <td>The #Kindle2 seems the best eReader, but will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>4</td>\n",
       "      <td>I have a google addiction. Thank you for point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2</td>\n",
       "      <td>@ruby_gem My primary debit card is Visa Electron.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2</td>\n",
       "      <td>Off to the bank to get my new visa platinum card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0</td>\n",
       "      <td>dearest @google, you rich bastards! the VISA c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2</td>\n",
       "      <td>has a date with bobby flay and gut fieri from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4</td>\n",
       "      <td>Excited about seeing Bobby Flay and Guy Fieri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>4</td>\n",
       "      <td>Gonna go see Bobby Flay 2moro at Shoreline. Ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>4</td>\n",
       "      <td>can't wait for the great american food and mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>4</td>\n",
       "      <td>My dad was in NY for a day, we ate at MESA gri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>Fighting with LaTex. Again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0</td>\n",
       "      <td>@Iheartseverus we love you too and don't want ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0</td>\n",
       "      <td>7 hours. 7 hours of inkscape crashing, normall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2</td>\n",
       "      <td>How to Track Iran with Social Media: http://bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0</td>\n",
       "      <td>Shit's hitting the fan in Iran...craziness ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>Monday already. Iran may implode. Kitchen is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2</td>\n",
       "      <td>Twitter Stock buzz: $AAPL $ES_F $SPY $SPX $PAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4</td>\n",
       "      <td>getting ready to test out some burger receipes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>2</td>\n",
       "      <td>@johncmayer is Bobby Flay joining you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>4</td>\n",
       "      <td>i lam so in love with Bobby Flay... he is my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0</td>\n",
       "      <td>I just created my first LaTeX file from scratc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>4</td>\n",
       "      <td>using Linux and loving it - so much nicer than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>4</td>\n",
       "      <td>After using LaTeX a lot, any other typeset mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     polarity                                              tweet\n",
       "0           4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1           4  Reading my kindle2...  Love it... Lee childs i...\n",
       "2           4  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3           4  @kenburbary You'll love your Kindle2. I've had...\n",
       "4           4  @mikefish  Fair enough. But i have the Kindle2...\n",
       "5           4  @richardebaker no. it is too big. I'm quite ha...\n",
       "6           0  Fuck this economy. I hate aig and their non lo...\n",
       "7           4                      Jquery is my new best friend.\n",
       "8           4                                      Loves twitter\n",
       "9           4  how can you not love Obama? he makes jokes abo...\n",
       "10          2  Check this video out -- President Obama at the...\n",
       "11          0  @Karoli I firmly believe that Obama/Pelosi hav...\n",
       "12          4  House Correspondents dinner was last night who...\n",
       "13          4  Watchin Espn..Jus seen this new Nike Commerica...\n",
       "14          0  dear nike, stop with the flywire. that shit is...\n",
       "15          4  #lebron best athlete of our generation, if not...\n",
       "16          0  I was talking to this guy last night and he wa...\n",
       "17          4                 i love lebron. http://bit.ly/PdHur\n",
       "18          0  @ludajuice Lebron is a Beast, but I'm still ch...\n",
       "19          4                        @Pmillzz lebron IS THE BOSS\n",
       "20          4  @sketchbug Lebron is a hometown hero to me, lo...\n",
       "21          4        lebron and zydrunas are such an awesome duo\n",
       "22          4  @wordwhizkid Lebron is a beast... nobody in th...\n",
       "23          4  downloading apps for my iphone! So much fun :-...\n",
       "24          4  good news, just had a call from the Visa offic...\n",
       "25          4  http://twurl.nl/epkr4b - awesome come back fro...\n",
       "26          4  In montreal for a long weekend of R&amp;R. Muc...\n",
       "27          4  Booz Allen Hamilton has a bad ass homegrown so...\n",
       "28          4  [#MLUC09] Customer Innovation Award Winner: Bo...\n",
       "29          4  @SoChi2 I current use the Nikon D90 and love i...\n",
       "..        ...                                                ...\n",
       "468         0  Man I kinda dislike Apple right now. Case in p...\n",
       "469         4  @cwong08 I have a Kindle2 (&amp; Sony PRS-500)...\n",
       "470         4  The #Kindle2 seems the best eReader, but will ...\n",
       "471         4  I have a google addiction. Thank you for point...\n",
       "472         2  @ruby_gem My primary debit card is Visa Electron.\n",
       "473         2   Off to the bank to get my new visa platinum card\n",
       "474         0  dearest @google, you rich bastards! the VISA c...\n",
       "475         2  has a date with bobby flay and gut fieri from ...\n",
       "476         4  Excited about seeing Bobby Flay and Guy Fieri ...\n",
       "477         4  Gonna go see Bobby Flay 2moro at Shoreline. Ea...\n",
       "478         4  can't wait for the great american food and mus...\n",
       "479         4  My dad was in NY for a day, we ate at MESA gri...\n",
       "480         0                      Fighting with LaTex. Again...\n",
       "481         0  @Iheartseverus we love you too and don't want ...\n",
       "482         0  7 hours. 7 hours of inkscape crashing, normall...\n",
       "483         2  How to Track Iran with Social Media: http://bi...\n",
       "484         0  Shit's hitting the fan in Iran...craziness ind...\n",
       "485         0  Monday already. Iran may implode. Kitchen is a...\n",
       "486         2  Twitter Stock buzz: $AAPL $ES_F $SPY $SPX $PAL...\n",
       "487         4  getting ready to test out some burger receipes...\n",
       "488         2             @johncmayer is Bobby Flay joining you?\n",
       "489         4  i lam so in love with Bobby Flay... he is my f...\n",
       "490         0  I just created my first LaTeX file from scratc...\n",
       "491         4  using Linux and loving it - so much nicer than...\n",
       "492         4  After using LaTeX a lot, any other typeset mat...\n",
       "493         2  Ask Programming: LaTeX or InDesign?: submitted...\n",
       "494         0  On that note, I hate Word. I hate Pages. I hat...\n",
       "495         4  Ahhh... back in a *real* text editing environm...\n",
       "496         0  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
       "497         0  Reading the tweets coming out of Iran... The w...\n",
       "\n",
       "[498 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df[['polarity', 'tweet']]\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Twitter word embedding:\n",
    "\n",
    "I apply lower cases for all tweets because word embedding pre-trained data has lower characters.\n",
    "\n",
    "2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brain/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df2['tweet'] = df2['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data with .75 and .25 rate, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('trainingandtestdata/training.1600000.processed.noemoticon.csv')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tokenization, Punctuation, Stopwords\n",
    "\n",
    "**Stopwords:** There are many non-informative words in the texts which occur more than once in almost every document such as articles, prepositions and conjunctions. These are called stop words. For the purpose of filtering these words,which have no contribution to the classification task, a stop word list is composed [1].\n",
    "\n",
    "**Tokenization:** Tokenization is the task of dividing a sequence of words or a document unit into pieces.\n",
    "\n",
    "**Punctuation:** Remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy() # copy the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenized_text = []\n",
    "punctuated_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct tweets as N-gram\n",
    "N-grams are sequences of letters or words extracted from documents. I use word N-grams. Common values used for n are 1, 2 or 3 for word n-grams. In this project, I select both unigram and bigram and merge them. The main idea behind this method is that words which are similar to each other will have a high proportion of N-grams [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brain/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(df3)):\n",
    "    tokenized_text.append([x.encode('UTF8') for x in tknzr.tokenize(df2['tweet'][index])])    \n",
    "    punctuated_text.append(filter(lambda x: x not in string.punctuation, tokenized_text[index]))    \n",
    "    cleaned_text = filter(lambda y: y not in stop_words, punctuated_text[index])\n",
    "    cleaned_text = \", \".join(cleaned_text)\n",
    "    \n",
    "    df3['tweet'][index] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@stellargirl, loooooooovvvvvveee, kindle, 2, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>reading, kindle, 2, ..., love, ..., lee, child...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ok, first, assesment, #kindle2, ..., fucking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@kenburbary, love, kindle, 2, i've, mine, mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish, fair, enough, kindle, 2, think, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>@richardebaker, big, i'm, quite, happy, kindle, 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>fuck, economy, hate, aig, non, loan, given, asses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>jquery, new, best, friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>loves, twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>love, obama, makes, jokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>check, video, president, obama, white, house, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>@karoli, firmly, believe, obama, pelosi, zero,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>house, correspondents, dinner, last, night, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>watchin, espn, .., jus, seen, new, nike, comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>dear, nike, stop, flywire, shit, waste, scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>#lebron, best, athlete, generation, time, bask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>talking, guy, last, night, telling, die, hard,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>love, lebron, http://bit.ly/pdhur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>@ludajuice, lebron, beast, i'm, still, cheerin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>@pmillzz, lebron, boss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>@sketchbug, lebron, hometown, hero, lol, love,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>lebron, zydrunas, awesome, duo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>@wordwhizkid, lebron, beast, ..., nobody, nba,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>downloading, apps, iphone, much, fun, :-), lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>good, news, call, visa, office, saying, everyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>http://twurl.nl/epkr4b, awesome, come, back, @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>montreal, long, weekend, r, r, much, needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>booz, allen, hamilton, bad, ass, homegrown, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>#mluc09, customer, innovation, award, winner, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>@sochi2, current, use, nikon, d90, love, much,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>man, kinda, dislike, apple, right, case, point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>4</td>\n",
       "      <td>@cwong08, kindle, 2, sony, prs, 500, like, phy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>4</td>\n",
       "      <td>#kindle2, seems, best, ereader, work, uk, get,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>4</td>\n",
       "      <td>google, addiction, thank, pointing, @annamarti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2</td>\n",
       "      <td>@ruby_gem, primary, debit, card, visa, electron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2</td>\n",
       "      <td>bank, get, new, visa, platinum, card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0</td>\n",
       "      <td>dearest, @google, rich, bastards, visa, card, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2</td>\n",
       "      <td>date, bobby, flay, gut, fieri, food, network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4</td>\n",
       "      <td>excited, seeing, bobby, flay, guy, fieri, tomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>4</td>\n",
       "      <td>gonna, go, see, bobby, flay, 2moro, shoreline,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>4</td>\n",
       "      <td>can't, wait, great, american, food, music, fes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>4</td>\n",
       "      <td>dad, ny, day, ate, mesa, grill, last, night, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>fighting, latex, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0</td>\n",
       "      <td>@iheartseverus, love, want, die, latex, devil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0</td>\n",
       "      <td>7, hours, 7, hours, inkscape, crashing, normal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2</td>\n",
       "      <td>track, iran, social, media, http://bit.ly/2boqu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0</td>\n",
       "      <td>shit's, hitting, fan, iran, ..., craziness, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>monday, already, iran, may, implode, kitchen, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2</td>\n",
       "      <td>twitter, stock, buzz, aapl, es_f, spy, spx, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4</td>\n",
       "      <td>getting, ready, test, burger, receipes, weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>2</td>\n",
       "      <td>@johncmayer, bobby, flay, joining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>4</td>\n",
       "      <td>lam, love, bobby, flay, ..., favorite, rt, @te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0</td>\n",
       "      <td>created, first, latex, file, scratch, work, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>4</td>\n",
       "      <td>using, linux, loving, much, nicer, windows, .....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>4</td>\n",
       "      <td>using, latex, lot, typeset, mathematics, looks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>ask, programming, latex, indesign, submitted, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>note, hate, word, hate, pages, hate, latex, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>ahhh, ..., back, real, text, editing, environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>trouble, iran, see, hmm, iran, iran, far, away...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>reading, tweets, coming, iran, ..., whole, thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     polarity                                              tweet\n",
       "0           4  @stellargirl, loooooooovvvvvveee, kindle, 2, d...\n",
       "1           4  reading, kindle, 2, ..., love, ..., lee, child...\n",
       "2           4  ok, first, assesment, #kindle2, ..., fucking, ...\n",
       "3           4  @kenburbary, love, kindle, 2, i've, mine, mont...\n",
       "4           4  @mikefish, fair, enough, kindle, 2, think, per...\n",
       "5           4  @richardebaker, big, i'm, quite, happy, kindle, 2\n",
       "6           0  fuck, economy, hate, aig, non, loan, given, asses\n",
       "7           4                          jquery, new, best, friend\n",
       "8           4                                     loves, twitter\n",
       "9           4                          love, obama, makes, jokes\n",
       "10          2  check, video, president, obama, white, house, ...\n",
       "11          0  @karoli, firmly, believe, obama, pelosi, zero,...\n",
       "12          4  house, correspondents, dinner, last, night, wh...\n",
       "13          4  watchin, espn, .., jus, seen, new, nike, comme...\n",
       "14          0  dear, nike, stop, flywire, shit, waste, scienc...\n",
       "15          4  #lebron, best, athlete, generation, time, bask...\n",
       "16          0  talking, guy, last, night, telling, die, hard,...\n",
       "17          4                  love, lebron, http://bit.ly/pdhur\n",
       "18          0  @ludajuice, lebron, beast, i'm, still, cheerin...\n",
       "19          4                             @pmillzz, lebron, boss\n",
       "20          4  @sketchbug, lebron, hometown, hero, lol, love,...\n",
       "21          4                     lebron, zydrunas, awesome, duo\n",
       "22          4  @wordwhizkid, lebron, beast, ..., nobody, nba,...\n",
       "23          4  downloading, apps, iphone, much, fun, :-), lit...\n",
       "24          4  good, news, call, visa, office, saying, everyt...\n",
       "25          4  http://twurl.nl/epkr4b, awesome, come, back, @...\n",
       "26          4        montreal, long, weekend, r, r, much, needed\n",
       "27          4  booz, allen, hamilton, bad, ass, homegrown, so...\n",
       "28          4  #mluc09, customer, innovation, award, winner, ...\n",
       "29          4  @sochi2, current, use, nikon, d90, love, much,...\n",
       "..        ...                                                ...\n",
       "468         0  man, kinda, dislike, apple, right, case, point...\n",
       "469         4  @cwong08, kindle, 2, sony, prs, 500, like, phy...\n",
       "470         4  #kindle2, seems, best, ereader, work, uk, get,...\n",
       "471         4  google, addiction, thank, pointing, @annamarti...\n",
       "472         2    @ruby_gem, primary, debit, card, visa, electron\n",
       "473         2               bank, get, new, visa, platinum, card\n",
       "474         0  dearest, @google, rich, bastards, visa, card, ...\n",
       "475         2       date, bobby, flay, gut, fieri, food, network\n",
       "476         4  excited, seeing, bobby, flay, guy, fieri, tomo...\n",
       "477         4  gonna, go, see, bobby, flay, 2moro, shoreline,...\n",
       "478         4  can't, wait, great, american, food, music, fes...\n",
       "479         4  dad, ny, day, ate, mesa, grill, last, night, m...\n",
       "480         0                               fighting, latex, ...\n",
       "481         0      @iheartseverus, love, want, die, latex, devil\n",
       "482         0  7, hours, 7, hours, inkscape, crashing, normal...\n",
       "483         2    track, iran, social, media, http://bit.ly/2boqu\n",
       "484         0  shit's, hitting, fan, iran, ..., craziness, in...\n",
       "485         0  monday, already, iran, may, implode, kitchen, ...\n",
       "486         2  twitter, stock, buzz, aapl, es_f, spy, spx, pa...\n",
       "487         4  getting, ready, test, burger, receipes, weeken...\n",
       "488         2                  @johncmayer, bobby, flay, joining\n",
       "489         4  lam, love, bobby, flay, ..., favorite, rt, @te...\n",
       "490         0  created, first, latex, file, scratch, work, we...\n",
       "491         4  using, linux, loving, much, nicer, windows, .....\n",
       "492         4  using, latex, lot, typeset, mathematics, looks...\n",
       "493         2  ask, programming, latex, indesign, submitted, ...\n",
       "494         0  note, hate, word, hate, pages, hate, latex, sa...\n",
       "495         4  ahhh, ..., back, real, text, editing, environm...\n",
       "496         0  trouble, iran, see, hmm, iran, iran, far, away...\n",
       "497         0  reading, tweets, coming, iran, ..., whole, thi...\n",
       "\n",
       "[498 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge bigrams to unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brain/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(df3)):\n",
    "    sentence = df3['tweet'][index]\n",
    "    n = 2\n",
    "    grams_list = []\n",
    "    bigrams = ngrams(sentence.split(), n)\n",
    "    for grams in bigrams:\n",
    "        grams = ', {}_{}'.format(grams[0].replace(',', ''), grams[1].replace(',', ''))\n",
    "        df3['tweet'][index] = df3['tweet'][index] + grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      @stellargirl, loooooooovvvvvveee, kindle, 2, d...\n",
       "1      reading, kindle, 2, ..., love, ..., lee, child...\n",
       "2      ok, first, assesment, #kindle2, ..., fucking, ...\n",
       "3      @kenburbary, love, kindle, 2, i've, mine, mont...\n",
       "4      @mikefish, fair, enough, kindle, 2, think, per...\n",
       "5      @richardebaker, big, i'm, quite, happy, kindle...\n",
       "6      fuck, economy, hate, aig, non, loan, given, as...\n",
       "7      jquery, new, best, friend, jquery_new, new_bes...\n",
       "8                          loves, twitter, loves_twitter\n",
       "9      love, obama, makes, jokes, love_obama, obama_m...\n",
       "10     check, video, president, obama, white, house, ...\n",
       "11     @karoli, firmly, believe, obama, pelosi, zero,...\n",
       "12     house, correspondents, dinner, last, night, wh...\n",
       "13     watchin, espn, .., jus, seen, new, nike, comme...\n",
       "14     dear, nike, stop, flywire, shit, waste, scienc...\n",
       "15     #lebron, best, athlete, generation, time, bask...\n",
       "16     talking, guy, last, night, telling, die, hard,...\n",
       "17     love, lebron, http://bit.ly/pdhur, love_lebron...\n",
       "18     @ludajuice, lebron, beast, i'm, still, cheerin...\n",
       "19     @pmillzz, lebron, boss, @pmillzz_lebron, lebro...\n",
       "20     @sketchbug, lebron, hometown, hero, lol, love,...\n",
       "21     lebron, zydrunas, awesome, duo, lebron_zydruna...\n",
       "22     @wordwhizkid, lebron, beast, ..., nobody, nba,...\n",
       "23     downloading, apps, iphone, much, fun, :-), lit...\n",
       "24     good, news, call, visa, office, saying, everyt...\n",
       "25     http://twurl.nl/epkr4b, awesome, come, back, @...\n",
       "26     montreal, long, weekend, r, r, much, needed, m...\n",
       "27     booz, allen, hamilton, bad, ass, homegrown, so...\n",
       "28     #mluc09, customer, innovation, award, winner, ...\n",
       "29     @sochi2, current, use, nikon, d90, love, much,...\n",
       "                             ...                        \n",
       "468    man, kinda, dislike, apple, right, case, point...\n",
       "469    @cwong08, kindle, 2, sony, prs, 500, like, phy...\n",
       "470    #kindle2, seems, best, ereader, work, uk, get,...\n",
       "471    google, addiction, thank, pointing, @annamarti...\n",
       "472    @ruby_gem, primary, debit, card, visa, electro...\n",
       "473    bank, get, new, visa, platinum, card, bank_get...\n",
       "474    dearest, @google, rich, bastards, visa, card, ...\n",
       "475    date, bobby, flay, gut, fieri, food, network, ...\n",
       "476    excited, seeing, bobby, flay, guy, fieri, tomo...\n",
       "477    gonna, go, see, bobby, flay, 2moro, shoreline,...\n",
       "478    can't, wait, great, american, food, music, fes...\n",
       "479    dad, ny, day, ate, mesa, grill, last, night, m...\n",
       "480      fighting, latex, ..., fighting_latex, latex_...\n",
       "481    @iheartseverus, love, want, die, latex, devil,...\n",
       "482    7, hours, 7, hours, inkscape, crashing, normal...\n",
       "483    track, iran, social, media, http://bit.ly/2boq...\n",
       "484    shit's, hitting, fan, iran, ..., craziness, in...\n",
       "485    monday, already, iran, may, implode, kitchen, ...\n",
       "486    twitter, stock, buzz, aapl, es_f, spy, spx, pa...\n",
       "487    getting, ready, test, burger, receipes, weeken...\n",
       "488    @johncmayer, bobby, flay, joining, @johncmayer...\n",
       "489    lam, love, bobby, flay, ..., favorite, rt, @te...\n",
       "490    created, first, latex, file, scratch, work, we...\n",
       "491    using, linux, loving, much, nicer, windows, .....\n",
       "492    using, latex, lot, typeset, mathematics, looks...\n",
       "493    ask, programming, latex, indesign, submitted, ...\n",
       "494    note, hate, word, hate, pages, hate, latex, sa...\n",
       "495    ahhh, ..., back, real, text, editing, environm...\n",
       "496    trouble, iran, see, hmm, iran, iran, far, away...\n",
       "497    reading, tweets, coming, iran, ..., whole, thi...\n",
       "Name: tweet, Length: 498, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features by using Tf-idf\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@stellargirl, loooooooovvvvvveee, kindle, 2, dx, cool, 2, fantastic, right, @stellargirl_loooooooovvvvvveee, loooooooovvvvvveee_kindle, kindle_2, 2_dx, dx_cool, cool_2, 2_fantastic, fantastic_right',\n",
       " 'reading, kindle, 2, ..., love, ..., lee, childs, good, read, reading_kindle, kindle_2, 2_..., ..._love, love_..., ..._lee, lee_childs, childs_good, good_read',\n",
       " 'ok, first, assesment, #kindle2, ..., fucking, rocks, ok_first, first_assesment, assesment_#kindle2, #kindle2_..., ..._fucking, fucking_rocks',\n",
       " \"@kenburbary, love, kindle, 2, i've, mine, months, never, looked, back, new, big, one, huge, need, remorse, :), @kenburbary_love, love_kindle, kindle_2, 2_i've, i've_mine, mine_months, months_never, never_looked, looked_back, back_new, new_big, big_one, one_huge, huge_need, need_remorse, remorse_:)\",\n",
       " '@mikefish, fair, enough, kindle, 2, think, perfect, :), @mikefish_fair, fair_enough, enough_kindle, kindle_2, 2_think, think_perfect, perfect_:)',\n",
       " \"@richardebaker, big, i'm, quite, happy, kindle, 2, @richardebaker_big, big_i'm, i'm_quite, quite_happy, happy_kindle, kindle_2\",\n",
       " 'fuck, economy, hate, aig, non, loan, given, asses, fuck_economy, economy_hate, hate_aig, aig_non, non_loan, loan_given, given_asses',\n",
       " 'jquery, new, best, friend, jquery_new, new_best, best_friend',\n",
       " 'loves, twitter, loves_twitter',\n",
       " 'love, obama, makes, jokes, love_obama, obama_makes, makes_jokes']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [x for x in df3['tweet']]\n",
    "corpus[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'childs': 2.7047480922384253, u'kindle2': 2.7047480922384253, u'new_best': 2.7047480922384253, u'_fucking': 2.7047480922384253, u'good_read': 2.7047480922384253, u'ok_first': 2.7047480922384253, u'hate': 2.7047480922384253, u'cool_2': 2.7047480922384253, u'perfect': 2.7047480922384253, u'mikefish': 2.7047480922384253, u'_love': 2.7047480922384253, u'fucking_rocks': 2.7047480922384253, u'aig_non': 2.7047480922384253, u'good': 2.7047480922384253, u'read': 2.7047480922384253, u'big': 2.2992829841302607, u'2_': 2.7047480922384253, u'dx': 2.7047480922384253, u'kindle2_': 2.7047480922384253, u'cool': 2.7047480922384253, u'hate_aig': 2.7047480922384253, u'perfect_': 2.7047480922384253, u'happy_kindle': 2.7047480922384253, u'rocks': 2.7047480922384253, u'jokes': 2.7047480922384253, u'non_loan': 2.7047480922384253, u'right': 2.7047480922384253, u'fair': 2.7047480922384253, u'fucking': 2.7047480922384253, u'twitter': 2.7047480922384253, u'back': 2.7047480922384253, u'remorse_': 2.7047480922384253, u'given_asses': 2.7047480922384253, u'_lee': 2.7047480922384253, u'2_fantastic': 2.7047480922384253, u'2_think': 2.7047480922384253, u'best': 2.7047480922384253, u'assesment_': 2.7047480922384253, u'loooooooovvvvvveee_kindle': 2.7047480922384253, u'enough': 2.7047480922384253, u'remorse': 2.7047480922384253, u'think_perfect': 2.7047480922384253, u'new': 2.2992829841302607, u'loves_twitter': 2.7047480922384253, u'reading': 2.7047480922384253, u'lee': 2.7047480922384253, u'loan_given': 2.7047480922384253, u'never': 2.7047480922384253, u'quite': 2.7047480922384253, u've_mine': 2.7047480922384253, u'stellargirl': 2.7047480922384253, u'dx_cool': 2.7047480922384253, u'ok': 2.7047480922384253, u'best_friend': 2.7047480922384253, u'fantastic_right': 2.7047480922384253, u'obama_makes': 2.7047480922384253, u'richardebaker_big': 2.7047480922384253, u'makes': 2.7047480922384253, u'think': 2.7047480922384253, u'first': 2.7047480922384253, u'fantastic': 2.7047480922384253, u'love': 2.0116009116784799, u'loan': 2.7047480922384253, u'new_big': 2.7047480922384253, u'one': 2.7047480922384253, u'mikefish_fair': 2.7047480922384253, u'non': 2.7047480922384253, u'kenburbary_love': 2.7047480922384253, u'richardebaker': 2.7047480922384253, u'fair_enough': 2.7047480922384253, u'childs_good': 2.7047480922384253, u'lee_childs': 2.7047480922384253, u'given': 2.7047480922384253, u'kenburbary': 2.7047480922384253, u'quite_happy': 2.7047480922384253, u'big_i': 2.7047480922384253, u'asses': 2.7047480922384253, u'one_huge': 2.7047480922384253, u'friend': 2.7047480922384253, u'happy': 2.7047480922384253, u'huge_need': 2.7047480922384253, u'mine': 2.7047480922384253, u'love_': 2.7047480922384253, u'big_one': 2.7047480922384253, u'obama': 2.7047480922384253, u'jquery': 2.7047480922384253, u'reading_kindle': 2.7047480922384253, u'2_i': 2.7047480922384253, u'loves': 2.7047480922384253, u'stellargirl_loooooooovvvvvveee': 2.7047480922384253, u'kindle': 1.6061358035703155, u'aig': 2.7047480922384253, u'months': 2.7047480922384253, u've': 2.7047480922384253, u'2_dx': 2.7047480922384253, u'looked_back': 2.7047480922384253, u'fuck': 2.7047480922384253, u'need': 2.7047480922384253, u'loooooooovvvvvveee': 2.7047480922384253, u'economy_hate': 2.7047480922384253, u'kindle_2': 1.6061358035703155, u'huge': 2.7047480922384253, u'love_obama': 2.7047480922384253, u'jquery_new': 2.7047480922384253, u'never_looked': 2.7047480922384253, u'fuck_economy': 2.7047480922384253, u'need_remorse': 2.7047480922384253, u'assesment': 2.7047480922384253, u'mine_months': 2.7047480922384253, u'makes_jokes': 2.7047480922384253, u'economy': 2.7047480922384253, u'first_assesment': 2.7047480922384253, u'months_never': 2.7047480922384253, u'love_kindle': 2.7047480922384253, u'm_quite': 2.7047480922384253, u'enough_kindle': 2.7047480922384253, u'looked': 2.7047480922384253, u'back_new': 2.7047480922384253}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus[:10])\n",
    "idf = vectorizer.idf_\n",
    "\n",
    "print dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2_', u'2_dx', u'2_fantastic', u'2_i', u'2_think', u'_fucking', u'_lee', u'_love', u'aig', u'aig_non', u'asses', u'assesment', u'assesment_', u'back', u'back_new', u'best', u'best_friend', u'big', u'big_i', u'big_one', u'childs', u'childs_good', u'cool', u'cool_2', u'dx', u'dx_cool', u'economy', u'economy_hate', u'enough', u'enough_kindle', u'fair', u'fair_enough', u'fantastic', u'fantastic_right', u'first', u'first_assesment', u'friend', u'fuck', u'fuck_economy', u'fucking', u'fucking_rocks', u'given', u'given_asses', u'good', u'good_read', u'happy', u'happy_kindle', u'hate', u'hate_aig', u'huge', u'huge_need', u'jokes', u'jquery', u'jquery_new', u'kenburbary', u'kenburbary_love', u'kindle', u'kindle2', u'kindle2_', u'kindle_2', u'lee', u'lee_childs', u'loan', u'loan_given', u'looked', u'looked_back', u'loooooooovvvvvveee', u'loooooooovvvvvveee_kindle', u'love', u'love_', u'love_kindle', u'love_obama', u'loves', u'loves_twitter', u'm_quite', u'makes', u'makes_jokes', u'mikefish', u'mikefish_fair', u'mine', u'mine_months', u'months', u'months_never', u'need', u'need_remorse', u'never', u'never_looked', u'new', u'new_best', u'new_big', u'non', u'non_loan', u'obama', u'obama_makes', u'ok', u'ok_first', u'one', u'one_huge', u'perfect', u'perfect_', u'quite', u'quite_happy', u'read', u'reading', u'reading_kindle', u'remorse', u'remorse_', u'richardebaker', u'richardebaker_big', u'right', u'rocks', u'stellargirl', u'stellargirl_loooooooovvvvvveee', u'think', u'think_perfect', u'twitter', u've', u've_mine']\n",
      "shape: (10, 118)\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()\n",
    "print \"shape:\", X.toarray().shape\n",
    "#print X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "**fit_transform(X[, y]):** Fit the model with X and apply the dimensionality reduction on X. <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_ratio_:  [  1.23632427e-01   1.16407456e-01   1.14912259e-01   1.12887316e-01\n",
      "   1.12887316e-01   1.07912517e-01   1.06551974e-01   1.03761082e-01\n",
      "   1.01047653e-01   9.98816534e-33]\n",
      "singular_values:  [  1.04651058e+00   1.01547170e+00   1.00892900e+00   1.00000000e+00\n",
      "   1.00000000e+00   9.77717384e-01   9.71534384e-01   9.58726366e-01\n",
      "   9.46107623e-01   2.97454324e-16]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=50, svd_solver='full')\n",
    "principalComponents = pca.fit_transform(X.toarray())\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "             #, columns = ['principal component 1', 'principal component 2'])\n",
    "    \n",
    "print \"explained_variance_ratio_: \", pca.explained_variance_ratio_\n",
    "print \"singular_values: \", pca.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.318329</td>\n",
       "      <td>-0.314108</td>\n",
       "      <td>-0.001672</td>\n",
       "      <td>-4.053855e-17</td>\n",
       "      <td>-3.689064e-15</td>\n",
       "      <td>0.513173</td>\n",
       "      <td>-0.611184</td>\n",
       "      <td>-0.147906</td>\n",
       "      <td>-0.122438</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.296854</td>\n",
       "      <td>0.223728</td>\n",
       "      <td>-0.328827</td>\n",
       "      <td>-5.998976e-15</td>\n",
       "      <td>-3.296520e-15</td>\n",
       "      <td>0.084619</td>\n",
       "      <td>0.067494</td>\n",
       "      <td>0.773808</td>\n",
       "      <td>-0.030125</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.392847</td>\n",
       "      <td>-0.247869</td>\n",
       "      <td>-0.097611</td>\n",
       "      <td>-5.530957e-01</td>\n",
       "      <td>-6.006262e-01</td>\n",
       "      <td>-0.132157</td>\n",
       "      <td>-0.016534</td>\n",
       "      <td>0.047552</td>\n",
       "      <td>-0.046888</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.207245</td>\n",
       "      <td>0.356022</td>\n",
       "      <td>0.293709</td>\n",
       "      <td>6.052741e-15</td>\n",
       "      <td>4.785524e-15</td>\n",
       "      <td>-0.439843</td>\n",
       "      <td>-0.123739</td>\n",
       "      <td>-0.132185</td>\n",
       "      <td>-0.609859</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.344710</td>\n",
       "      <td>-0.300264</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>-7.720761e-16</td>\n",
       "      <td>-2.569020e-15</td>\n",
       "      <td>0.224037</td>\n",
       "      <td>0.728307</td>\n",
       "      <td>-0.243694</td>\n",
       "      <td>-0.149544</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.427301</td>\n",
       "      <td>-0.139887</td>\n",
       "      <td>0.134751</td>\n",
       "      <td>3.392141e-15</td>\n",
       "      <td>2.034353e-15</td>\n",
       "      <td>-0.488394</td>\n",
       "      <td>-0.103142</td>\n",
       "      <td>-0.083745</td>\n",
       "      <td>0.618990</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.392847</td>\n",
       "      <td>-0.247869</td>\n",
       "      <td>-0.097611</td>\n",
       "      <td>-2.436097e-01</td>\n",
       "      <td>7.793080e-01</td>\n",
       "      <td>-0.132157</td>\n",
       "      <td>-0.016534</td>\n",
       "      <td>0.047552</td>\n",
       "      <td>-0.046888</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.285070</td>\n",
       "      <td>0.317287</td>\n",
       "      <td>0.712893</td>\n",
       "      <td>8.980694e-15</td>\n",
       "      <td>9.864530e-15</td>\n",
       "      <td>0.361896</td>\n",
       "      <td>0.092607</td>\n",
       "      <td>0.128488</td>\n",
       "      <td>0.240946</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.392847</td>\n",
       "      <td>-0.247869</td>\n",
       "      <td>-0.097611</td>\n",
       "      <td>7.967054e-01</td>\n",
       "      <td>-1.786818e-01</td>\n",
       "      <td>-0.132157</td>\n",
       "      <td>-0.016534</td>\n",
       "      <td>0.047552</td>\n",
       "      <td>-0.046888</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.130829</td>\n",
       "      <td>0.600830</td>\n",
       "      <td>-0.518251</td>\n",
       "      <td>-9.582074e-15</td>\n",
       "      <td>-1.919979e-15</td>\n",
       "      <td>0.140983</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>-0.437420</td>\n",
       "      <td>0.192693</td>\n",
       "      <td>9.406332e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2             3             4         5  \\\n",
       "0  0.318329 -0.314108 -0.001672 -4.053855e-17 -3.689064e-15  0.513173   \n",
       "1  0.296854  0.223728 -0.328827 -5.998976e-15 -3.296520e-15  0.084619   \n",
       "2 -0.392847 -0.247869 -0.097611 -5.530957e-01 -6.006262e-01 -0.132157   \n",
       "3  0.207245  0.356022  0.293709  6.052741e-15  4.785524e-15 -0.439843   \n",
       "4  0.344710 -0.300264  0.000232 -7.720761e-16 -2.569020e-15  0.224037   \n",
       "5  0.427301 -0.139887  0.134751  3.392141e-15  2.034353e-15 -0.488394   \n",
       "6 -0.392847 -0.247869 -0.097611 -2.436097e-01  7.793080e-01 -0.132157   \n",
       "7 -0.285070  0.317287  0.712893  8.980694e-15  9.864530e-15  0.361896   \n",
       "8 -0.392847 -0.247869 -0.097611  7.967054e-01 -1.786818e-01 -0.132157   \n",
       "9 -0.130829  0.600830 -0.518251 -9.582074e-15 -1.919979e-15  0.140983   \n",
       "\n",
       "          6         7         8             9  \n",
       "0 -0.611184 -0.147906 -0.122438  9.406332e-17  \n",
       "1  0.067494  0.773808 -0.030125  9.406332e-17  \n",
       "2 -0.016534  0.047552 -0.046888  9.406332e-17  \n",
       "3 -0.123739 -0.132185 -0.609859  9.406332e-17  \n",
       "4  0.728307 -0.243694 -0.149544  9.406332e-17  \n",
       "5 -0.103142 -0.083745  0.618990  9.406332e-17  \n",
       "6 -0.016534  0.047552 -0.046888  9.406332e-17  \n",
       "7  0.092607  0.128488  0.240946  9.406332e-17  \n",
       "8 -0.016534  0.047552 -0.046888  9.406332e-17  \n",
       "9 -0.000742 -0.437420  0.192693  9.406332e-17  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix_file = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"matrix.txt\", matrix_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "paper: http://crowdsourcing-class.org/assignments/downloads/pak-paroubek.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] C. Silvatt, B. Ribeirot, “The Importance of Stop Word Removal on Recall Values in Text Categorization”, In International Joint Conference on Neural Networks, V ol. 3 (2003), p. 1661-1666, 2003.\n",
    "\n",
    "[2] P Majumder, M Mitra, B. B. Chaudhuri, “N-gram: a language independent approach to IR and NLP”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
